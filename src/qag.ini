[general]
quiet: False
ignoreWarnings: True
# QG|AE
trainFor: QG
# text|chat
modelType: chat
# modes: test|norm|heavy 
mode: test

[qagTrainer]
addCustomTokens: False
# optimize only the model completion (answers/questions)
# see https://github.com/huggingface/trl/issues/426 && .../trl/pull/445
optimizeCompletion: True
# arg to compute_metrics: computeAccuracy (def)|bleu|meteor|rouge
metricFx: computeAccuracy
# True for custom metrics till the preprocessLogits f(x) is finished
injectMetric: False

# packs several examples into one training input. SFTTrainer uses an EOS
# token to separate examples. forced to False when trainForCompletionOnly = True
packing: False


[dataFormatter]
delim: ###
# we MUST begin with <s> and a space for the tokenizer to match the
# tokens for respTempl below with the ones produced in the full input
# text completion
textRespTempleAE: <s> ${delim} Here is a context verse. ${delim} Verse: <context> ${delim} Here are seven nouns and noun phrases in a comma-separated list that appear in this Bible verse: <answer>
textRespTempleQG: <s> ${delim} Question for the answer:
# chat completion
chatRespTempleAE: <s> ${delim} Extract any nouns, noun phrases, actions, key phrases, and lists that appear in the following context and return them separated by <sep>. ${delim} Verse: <context> ${delim} Nouns, noun phrases, actions, key phrases, and lists: <answer>
chatRespKeyAE: ${delim} Nouns, noun phrases, actions, key phrases, and lists:
chatRespTempleQG: <s> ${delim} Given the following context verse and answer, write a question for the answer. ${delim} Verse: <context> ${delim} Answer: <answer> ${delim} Question: <question>
chatRespKeyQG: ${delim} Question:

# dataFormat: (par|sen)((Hl|Tok)(In))?_(Out)
# e.g.: parHlSen_A, parHlAns_Q, sen_As
; dataFormat: sen_As
# manual|generate
sampleMode: generate
; evalToTrainRatio: 0.02
evalToTrainRatio: 0.002

[paths]
base: /models/llama-hf/7b-${general:modelType}
output: /models/output/${general:mode}/7b-${general:modelType}${general:trainFor}
data: /data/pbe/${general:trainFor}
log: /data/logs
# dataProcessor paths (specify files)
dpBibleSrc: /data/bible/nkjv.csv
dpSource: /data/pbe/clean/contextQuestions.csv
dpDest: /data/pbe/QG/data.json

[peft]
# parameter-efficient fine-tuning (PEFT) w/ low rank adapter (LoRA)
# trains the difference in weights Î”h on the side of feed-forward
# layers (FFW). makes for faster, lighter training. see https://rb.gy/9gor5
# https://huggingface.co/docs/peft/conceptual_guides/lora#common-lora-parameters-in-peft

# matrix rank: size of matrices "on the side" of FFW
# determines how many parameters get fine-tuned
r: 64
# scaling factor
loraAlpha: 16
# should bias be retained? vals: none|all|lora_only
bias: none
loraDropout: 0.1

# this sets up training
[trainArgs]
# refers to how large the batches are per each GPU
perDeviceTrainBatchSize: 2
gradientAccumulationSteps: 4
learningRate: 2e-4
# how many steps of training we want to do
maxTestSteps: 1
maxNormSteps: 1000
maxHeavySteps: 3000
# controls for how often to stop to evaluate|save|log the model
# no|steps|epoch / 1-maxSteps
saveAndEvalStrategy: steps
saveAndEvalSteps: 100

# output length
maxSeqLength: 512

# how many predictions steps to accumulate the output tensors for on the
# GPU, before moving the results to the CPU. If None all whole predictions
# are accumulated on GPU (faster but throws out of memory errors)
evalAccumulationSteps: 10

# number of most recent checkpoints to save
saveTotalLimit: 3
# whether to keep around the best model
loadBestModelAtEnd: True

# other potential settings
; num_train_epochs: specifies the number of epochs that should be performed by the trainer