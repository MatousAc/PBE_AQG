[general]
quiet: False
ignoreWarnings: True
# QG|AE
trainFor: AE

[qagTrainer]
# modes: test|norm|or heavy 
mode: test
addCustomTokens: False
# optimize only the model completion (answers/questions)
# see https://github.com/huggingface/trl/issues/426
# and https://github.com/huggingface/trl/pull/445
optimizeCompletion: True
# arg to compute_metrics: computeAccuracy (def)|bleu|meteor|rogue
# these are only for binary classification: precision
# don't change this until the preprocessLogits function is finished
metricFx: computeAccuracy

# packing lets us pack several examples into one training input
# SFTTrainer uses an EOS token to separate examples
# forced to False when trainForCompletionOnly = true
packing: False


[dataFormatter]
# dataFormat: (par|sen)((Hl|Tok)(In))?_(Out)
# e.g.: parHlSen_A, parHlAns_Q, sen_As
dataFormat: sen_As
promptDelim: ###
evalToTrainRatio: 0.001
aeMinAnswerCount: 2
# we MUST begin with <s> and a space for the tokenizer to match the
# tokens for respTempl below with the ones produced in the full input
# we Cannot add another leading space to this
respTempleAE: <s> ${promptDelim} Key nouns, actions, and phrases:
respTempleQG: <s> ${promptDelim} Question for the answer:

[dataProcessor]
# data processing mode: randomVerse|qgToAE|pbeClean|none
mode: pbeClean
dpSource: /home/ac/code/qag/data/squadBase
dpDest: /home/ac/code/qag/data/squadAE
; dpDest: ${dpSource}

[paths]
models: /home/ac/code/qag/models
base: ${models}/llama-hf/7b
output: ${models}/output/${qagTrainer:mode}/7bAEFromSquad01
log: ${output}/logs
data: /home/ac/code/qag/data/squadAE
; data: lmqg/qg_squad

[peft]
# parameter-efficient fine-tuning (PEFT) w/ low rank adapter (LoRA)
# trains the difference in weights Î”h on the side of feed-forward
# layers (FFW). makes for faster, lighter training. see https://rb.gy/9gor5
# https://huggingface.co/docs/peft/conceptual_guides/lora#common-lora-parameters-in-peft

# matrix rank: size of matrices "on the side" of FFW
# determines how many parameters get fine tuned
r: 64
# scaling factor
loraAlpha: 16
# should biaas be retained? vals: none|all|lora_only
bias: none
loraDropout: 0.1
# causal lm means the lm only sees tokens to the left of what it's predicting
taskType: CAUSAL_LM

# this sets up training
[trainArgs]
# refers to how large the batches are per each GPU
perDeviceTrainBatchSize: 2
gradientAccumulationSteps: 4
learningRate: 2e-4
# how many steps of training we want to do
maxTestSteps: 10
maxNormSteps: 1000
maxHeavySteps: 3000
# controls for how often to stop to evaluate|save|log the model
# no|steps|epoch / 1-maxSteps
saveStrategy: steps
saveSteps: 100
evaluationStrategy: steps
evalSteps: 5
; evalSteps: 100
# SFTTrainer automatically reports to wandb if
# it is installed. put 'none' below to turn off
reportTo: wandb
# output length
maxSeqLength: 512

# how many predictions steps to accumulate the output tensors for on the
# GPU, before moving the results to the CPU. If None all whole predictions
# are accumulated on GPU (faster but throws out of memory errors)
evalAccumulationSteps: 10

# number of most recent checkpoints to save
saveTotalLimit: 3
# whether to keep around the best model
loadBestModelAtEnd: True
# may need this to match the metricFx above?
metric_for_best_model:

# other potential settings
; * `num_train_epochs` specifies the number of epochs that should be performed by the trainer