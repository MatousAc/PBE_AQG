[general]
quiet: True
ignoreWarnings: True
# QG|AE
trainFor: AE
# text|chat
modelType: chat
modelSize: 7
# modes: test|norm 
mode: norm

[dataFormatter]
delim: ###
# we MUST begin with <s> and a space for the tokenizer to match the
# tokens for respTempl below with the ones produced in the full input
inputTempleAE: <s> ${delim} Extract potential answers to questions and return them separated by <sep>. ${delim} Verse: <context> ${delim} Potential answers: <answer>
respTempleAE: ${delim} Potential answers:
inputTempleQG: <s> ${delim} Write a question for the context and answer. ${delim} Verse: <context> ${delim} Answer: <answer> ${delim} Question: According to <question>
respTempleQG: ${delim} Question:

# manual|generate
sampleMode: generate
evalToTrainRatio: 0.07
qualityThreshold: 9

[paths]
base: /models/llama-hf/7b-${general:modelType}
output: /models/output/${general:mode}/${general:modelSize}b-${general:modelType}${general:trainFor}
data: /data/evaluation/auto/combined/
; data: /data/pbe/${general:trainFor}
log: /data/logs

# dataProcessor paths (generally you should specify files)
dpSource: /data/pbe/clean/contextQuestions.csv
; dpSource: /data/evaluation/manual/
dpDest: /data/evaluation/auto/combined/data.jsonl

# tunable hyperparameters
[hyperparameters]
# General hyperparameters
learningRate: 1e-4
weightDecay: 1e-4
# multiple epochs decrease performance due to overfitting
epochs: 1

# LORA
# scaling factor as compared to r. setting it at r means your data is as loud as the base data
# setting it at higher multiples makes your data louder. I need to test it at lower values
loraAlpha: 256
# r = matrix rank: size of matrices "on the side" of
# FFW determines how many parameters get fine-tuned
r: 64
loraDropout: 0.01
# which layers to train: query, value, key, o_proj. use first letters
loraLayers: qvko
# none|all|lora_only
bias: none

# general setting for training
[train]
# controls for how often to stop to evaluate|save|log the model
# no|steps|epoch
saveStrategy: epoch
evalStrategy: steps
stepSize: 50
testSteps: 2
# number of most recent checkpoints to save
saveTotalLimit: 1
# whether to keep around the best model
loadBestModelAtEnd: False

# output length
maxSeqLength: 512

# refers to how large the batches are per each GPU
perDeviceTrainBatchSize: 2
# how many predictions steps to accumulate the output tensors for on the GPU,
# before moving the results to the CPU. If None, all predictions are accumulated
# on GPU. higher numbers are faster but can throw 'Out of Memory' errors)
gradientAccumulationSteps: 10
evalAccumulationSteps: 10

# optimize only the model completion (answers/questions)
# see https://github.com/huggingface/trl/issues/426 && .../trl/pull/445
optimizeCompletion: True
# packs several examples into one training input. SFTTrainer uses an EOS
# token to separate examples. forced to False when trainForCompletionOnly = True
packing: False

# whether to use bleu, rouge, and meteor for eval instead of the default computeAccuracy
generativeEval: True
useEvalPred: True

addCustomTokens: False

[eval]
# bleu|meteor|rouge
evalMetric: bleu