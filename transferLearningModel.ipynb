{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with Transformers and PyTorch on the potsawee T5 model\n",
    "In this notebook I demonstrate how to perform transfer learning on an existing T5 model.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages and Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again I make sure that pyTorch is installed and I import generally useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally useful libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mashu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides installing the imported libraries above you must also install some others:\n",
    "\n",
    "`pip install kaggle rouge_score`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global setting variables (eventually should go into a .json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = ' <sep> '\n",
    "PBEDataSource = 'singlePointQuestions.csv'\n",
    "bibleDataSource = 'nkjv.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "For the training to occur, I need to create a dataset in my code that includes all the inputs and outputs of my model. These come from two different sources. Inputs are technically the verses in [`nkjv.csv`](nkjv.csv) and the outputs are in my [`singlePointQuestions.csv`](singlePointQuestions.csv). Below I define functions that put these two sources together into a pandas data frame."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I load the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book</th>\n",
       "      <th>ChapterNumber</th>\n",
       "      <th>VerseNumber</th>\n",
       "      <th>Verse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In the beginning God created the heavens and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The earth was without form, and void; and dark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Then God said, \"Let there be light\"; and there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>And God saw the light, that it was good; and G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>God called the light Day, and the darkness He ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Book  ChapterNumber  VerseNumber   \n",
       "0  Genesis              1            1  \\\n",
       "1  Genesis              1            2   \n",
       "2  Genesis              1            3   \n",
       "3  Genesis              1            4   \n",
       "4  Genesis              1            5   \n",
       "\n",
       "                                               Verse  \n",
       "0  In the beginning God created the heavens and t...  \n",
       "1  The earth was without form, and void; and dark...  \n",
       "2  Then God said, \"Let there be light\"; and there...  \n",
       "3  And God saw the light, that it was good; and G...  \n",
       "4  God called the light Day, and the darkness He ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nkjv = pd.read_csv(bibleDataSource)\n",
    "nkjv.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I define a function that will get any span of verses specified by parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And He said to him, \"Most assuredly, I say to you, hereafter you shall see heaven open, and the angels of God ascending and descending upon the Son of Man.\" On the third day there was a wedding in Cana of Galilee, and the mother of Jesus was there. Now both Jesus and His disciples were invited to the wedding. \n",
      "In the beginning God created the heavens and the earth. The earth was without form, and void; and darkness was on the face of the deep. And the Spirit of God was hovering over the face of the waters. Then God said, \"Let there be light\"; and there was light. \n",
      "But fornication and all uncleanness or covetousness, let it not even be named among you, as is fitting for saints; \n"
     ]
    }
   ],
   "source": [
    "# note that this function does not support verses across multiple books\n",
    "def getText(Book, StartChapter, StartVerse, EndChapter = None, EndVerse = None):\n",
    "  # default to start positions\n",
    "  EndChapter = EndChapter if EndChapter else StartChapter\n",
    "  EndVerse = EndVerse if EndVerse else StartVerse\n",
    "\n",
    "  text = \"\"\n",
    "  BookDf = nkjv[nkjv[\"Book\"] == Book]\n",
    "  for index, row in BookDf.iterrows():\n",
    "    chapter = row['ChapterNumber']\n",
    "    verse = row['VerseNumber']\n",
    "    if (chapter == StartChapter and verse >= StartVerse) or (chapter > StartChapter):\n",
    "      if chapter == EndChapter and verse > EndVerse:\n",
    "        break\n",
    "      text += row['Verse'] + \" \"\n",
    "  return text\n",
    "  \n",
    "\n",
    "print(getText(\"John\", 1, 51, 2, 2))   # John 1:51, 2:1-2\n",
    "print(getText(\"Genesis\", 1, 1, 1, 3)) # Genesis 1:1-3\n",
    "print(getText(\"Ephesians\", 5, 3))     # Ephesians 5:3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the function handles texts across multiple verses and chapters. It assumes no negative values or values too high (chapters and verses that don't exist).\n",
    "\n",
    "I also will need a simple helper function below to use df.apply later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextFromRawPBEData(r):\n",
    "  return getText(r['StartBook'], r['StartChapter'], r['StartVerse'], r['EndChapter'], r['EndVerse'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also need a function that creates a data frame ready for training. Define this function and explain how I know what my data needs to look like in the [Preprocessing Data](#preprocessing-data) section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Transfer Learning Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Format Research\n",
    "To know what format my data should take, I first try looking at the data format for other projects. I looked into the \"[Simplifying Paragraph-level Question Generation via Transformer Language Models](https://paperswithcode.com/paper/transformer-based-end-to-end-question)\" paper's hugging-face model as well as several T5 hugging-face packages for [Question Generation](https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap). As shown in [`transferLearningResearch.ipynb`](transferLearningResearch.ipynb) the [Q&A Generation](https://huggingface.co/potsawee/t5-large-generation-squad-QuestionAnswer) model by potsawee is the best model currently.\n",
    "\n",
    "Here I load the model and its tokenizer and define a function for generating questions and answers to be used for comparison later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potsawee_T5 is a model taken from https://huggingface.co/potsawee/t5-large-generation-squad-QuestionAnswer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "def potsaweeAQG(text):\n",
    "  inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "  outputs = model.generate(**inputs, max_length=100)\n",
    "  question_answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "  question_answer = question_answer.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n",
    "  return question_answer.split(tokenizer.sep_token)\n",
    "\n",
    "def getPotsaweeModel():\n",
    "  return AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I take a quick look at what the tokenizer does to the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And it was told the king of Jericho, saying, \"Behold, men have come here tonight from the children of Israel to search out the country.\"\n",
      "{'input_ids': [275, 34, 47, 1219, 8, 3, 1765, 13, 1022, 3723, 32, 6, 2145, 6, 96, 2703, 6134, 6, 1076, 43, 369, 270, 8988, 45, 8, 502, 13, 3352, 12, 960, 91, 8, 684, 535, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "joshua = nkjv[nkjv['Book'] == 'Joshua']\n",
    "joshua2 = joshua[joshua['ChapterNumber'] == 2]\n",
    "joshua2_2 = joshua2[joshua2[\"VerseNumber\"] == 2].iloc[0][\"Verse\"]\n",
    "print(joshua2_2)\n",
    "encoded_joshua2_2 = tokenizer(joshua2_2)\n",
    "print(encoded_joshua2_2)\n",
    "# print(json.dumps(encoded_joshua2_2, indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And it was told the king of Jericho, saying, \"Behold, men have come here tonight from the children of Israel to search out the country.\"</s>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_joshua2_2[\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, when we tokenize an input verse, we have the data encoded in the format that the model expects. It auto-adds a separator after the sentence: `</s>`\n",
    "\n",
    "It also appears that the data needs to be converted into a format similar to Word2Vec to be processed by the model.\n",
    "\n",
    "Before we do data pre-processing, I we need to know what python types and structures are being used for the fine-tuning APIs. I'm going to try to format my data in the way the datasets library expects it in [this tutorial](https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6Ijk2OTcxODA4Nzk2ODI5YTk3MmU3OWE5ZDFhOWZmZjExY2Q2MWIxZTMiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2ODE0MDU5MzEsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjEwNTYyNTc2NjM3ODUwNzExNDYyMiIsImVtYWlsIjoibWFzaHVhMjQ2OEBnbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiYXpwIjoiMjE2Mjk2MDM1ODM0LWsxazZxZTA2MHMydHAyYTJqYW00bGpkY21zMDBzdHRnLmFwcHMuZ29vZ2xldXNlcmNvbnRlbnQuY29tIiwibmFtZSI6Ik1hdG91xaEgSMO9YmwiLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EvQUdObXl4WXpTcmVXdUVpbUt4R1NYNTZ6aVJKUVhZa0FjbzRZM3MwZ0VFeHFQUT1zOTYtYyIsImdpdmVuX25hbWUiOiJNYXRvdcWhIiwiZmFtaWx5X25hbWUiOiJIw71ibCIsImlhdCI6MTY4MTQwNjIzMSwiZXhwIjoxNjgxNDA5ODMxLCJqdGkiOiIxOWViNzFkNmFmY2EzNWZhMDdlYzRlOTNjMDRiYjgxODIxYzA1ZDZiIn0.WUmK7KYoJP6FCtwllBH0p84wCHSLficCTTcxKi7fmEbsHIoLHDXPbe9LNCD3kjWJ9gL1rLVNY9MmW_OJW7IjgavYp2C5xJs3a86T8bNfGkCDpScs9_6C5I-kzUz99tOWYitPob5ydmUsgkUwmDOldMf3SIrrhbV5DTBjcaLYJVCVcGng39e6b2OoIor08_iG6eMY030fTFb-R51RUI5TCfJOHEOLDjCglXMfDdtTGRSqzvTTC0kiIERCCpz1OY3Xb6nbBvACMCP5WRmiofoFpxBJHMB1y5_BaT5QZRPpuR2hbAHd1HJUMsrMAINbd2ToCvUsaOZrL2wBfOxA9YPLtA) which is where my sample data comes from. If I can get my data into that same format then I can be confident that the API will accept it.\n",
    "\n",
    "To run the following code, you need to get the `'medium-articles.zip'` file. After setting up a kaggle account and placing your `kaggle.json` in the proper place, run `kaggle datasets download -d fabiochiusano/medium-articles` in your terminal in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/mashu/.cache/huggingface/datasets/csv/default-12543312f4be5eec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'text', 'url', 'authors', 'timestamp', 'tags'],\n",
      "        num_rows: 192368\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# look at an sample dataset to see what it looks like\n",
    "sample_data = load_dataset(\"csv\", data_files=\"medium-articles.zip\")\n",
    "print(sample_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Loading Function\n",
    "Looks like we have a DatasetDict format. Let's see if we can get our data to look the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Type                 Question Answer  NumberPoints StartBook   \n",
      "5750  bible-qna  How many were exempted?   None             1   1 Kings  \\\n",
      "\n",
      "      StartChapter  StartVerse  EndBook  EndChapter  EndVerse  \n",
      "5750            15          22  1 Kings          15        22  \n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Type', 'Question', 'Answer', 'NumberPoints', 'StartBook', 'StartChapter', 'StartVerse', 'EndBook', 'EndChapter', 'EndVerse'],\n",
      "        num_rows: 9983\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# some of the answers are literally \"None\". to keep pandas \n",
    "# from converting them to NaN, we need to set keep_default_na to False\n",
    "def getRawPBEData():\n",
    "  return pd.read_csv(PBEDataSource, keep_default_na=False)\n",
    "pbe_data = getRawPBEData()\n",
    "print(pbe_data[pbe_data['Question'] == \"How many were exempted?\"])\n",
    "# convert the rest of the way and look at structure\n",
    "pbe_data = Dataset.from_pandas(pbe_data)\n",
    "pbe_data = DatasetDict({\"train\": pbe_data})\n",
    "print(pbe_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Look at that. I think I have the data in the correct format now. Hopefully this will allow me to use the hugging-face training functions.\n",
    "\n",
    "##### Data Splitting Function\n",
    "At this point I will define a function that streamlines this data-loading from a dataframe and also splits the data up into a training, validation, and testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Type', 'Question', 'Answer', 'NumberPoints', 'StartBook', 'StartChapter', 'StartVerse', 'EndBook', 'EndChapter', 'EndVerse'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Type', 'Question', 'Answer', 'NumberPoints', 'StartBook', 'StartChapter', 'StartVerse', 'EndBook', 'EndChapter', 'EndVerse'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Type', 'Question', 'Answer', 'NumberPoints', 'StartBook', 'StartChapter', 'StartVerse', 'EndBook', 'EndChapter', 'EndVerse'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def dfToSplitDataDict(df):\n",
    "  # convert type\n",
    "  dataset = Dataset.from_pandas(df)\n",
    "  dataDict = DatasetDict({\"train\": dataset})\n",
    "  # split\n",
    "  train_test = dataDict[\"train\"].train_test_split(test_size=2000)\n",
    "  train_validation = train_test[\"train\"].train_test_split(test_size=1000)\n",
    "  dataDict[\"train\"] = train_validation[\"train\"]\n",
    "  dataDict[\"validation\"] = train_validation[\"test\"]\n",
    "  dataDict[\"test\"] = train_test[\"test\"]\n",
    "  \n",
    "  # shuffle\n",
    "  dataDict[\"train\"] = dataDict[\"train\"].shuffle().select(range(1000))\n",
    "  dataDict[\"validation\"] = dataDict[\"validation\"].shuffle().select(range(1000))\n",
    "  dataDict[\"test\"] = dataDict[\"test\"].shuffle().select(range(1000))\n",
    "\n",
    "  return dataDict\n",
    "\n",
    "# test function\n",
    "print(dfToSplitDataDict(getRawPBEData()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how in the function above I shuffle things. I do this for several reasons:\n",
    "* in case Tableau Prep Builder organized our data also, it is a good idea to shuffle \n",
    "* to separate out questions that are from different sources - some questions are from Babienco, Myaing, deFlutier, etc.\n",
    "* to ensure the question quality is about the same between training, validation, and testing data\n",
    "* make sure questions from different books are mixed up thoroughly - we don't want the model to remember context information from past questions, so feeding them in out of order might help that"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are about ready to pre-process. First I define a function that will generate `input_ids`, `attention_masks` and `labels` for each row. `input_ids` are the way the model/tokenizer keeps track of which words are which and `attention_masks` basically mark the important words. It seems to be an alternative to filtering stop words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Preprocess Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration\n",
    "Model-specific settings are very important for fine-tuning. Since I am fine-tuning a pre-existing model, it is important that I feed it the proper sizes of input, the correct command, and the correctly-formatted labels. Many of these configurations are taken from the values specified in the [`original_config`](./original_config/) folder which contains configuration files for the potsawee T5 model.\n",
    "\n",
    "Notice the `command` variable. The command tells T5 what to do. The form of the command is minimal. Examples usually show commands such as `\"summarize:\"` or `\"translate to german:\"`. The command is not specified in the model's documentation. The [`config.json`](./original_config/config.json) file contains a `\"summarize:\"` prefix under the `task_specific_params` key, so that may be work trying out. Other wise something like `\"generate question and answer:\"` will be tested.\n",
    "\n",
    "`max_verse_length`, and `max_qa_length` variables specify how long the input and output can be, aka. the largest input size and output size. In this case I am using `512` for the maximum input size for verses because it appears several times in the configuration. Otherwise there are also max lengths of `200` and `300`. If training the model takes too long, decreasing this size may be useful. The output length, `max_qa_length`, is set at `128` as no value is provided in the config.\n",
    "\n",
    "##### Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command = \"generate question and answer: \"\n",
    "command = \"summarize: \"\n",
    "max_verse_length = 512\n",
    "max_qa_length = 128\n",
    "\n",
    "def tokenize(data):\n",
    "  inputs = [command + text for text in data[\"context\"]]\n",
    "  model_inputs = tokenizer(inputs, max_length=max_verse_length, truncation=True)\n",
    "\n",
    "  # Setup the potsawee_tokenizer for targets\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    qa = tokenizer(data[\"qa\"], max_length=max_qa_length, truncation=True)\n",
    "\n",
    "  model_inputs[\"labels\"] = qa[\"input_ids\"]\n",
    "  return model_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Tokens\n",
    "One very interesting question is how the model is able to generate both a question and answer. All NLG pipelines generally produce a single stream of text as output, not two separate outputs.\n",
    "\n",
    "According to the [documentation online](https://huggingface.co/potsawee/t5-large-generation-squad-QuestionAnswer) the question and answer are output in the same string with the special added `<sep>` token separating the two (see [added_tokens.json](./original_config/added_tokens.json)). Thus, to continue training, this token must be inserted between the question and answer in \"labels.\"\n",
    "\n",
    "Documentation:\n",
    "* **Input**: context (e.g. news article)\n",
    "* **Output**: question `<sep>` answer\n",
    "\n",
    "##### Training Data Formatting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes a dataframe passed to it in the PBE question format standardized\n",
    "# by https://pbeprep.com and turns it into the format that the model expects\n",
    "\n",
    "def qaCombine(r):\n",
    "  return r['Question'] + sep + r['Answer']\n",
    "\n",
    "def rawPBEToTrainingData(pbeData):\n",
    "  result = pd.DataFrame()\n",
    "  result[\"context\"] = pbeData.apply(getTextFromRawPBEData, axis=1)\n",
    "  result[\"qa\"] = pbeData.apply(qaCombine, axis=1)\n",
    "  return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the pieces in place let's test what our training data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>qa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Now it came to pass in the days of Ahasuerus (...</td>\n",
       "      <td>Which Ahasuerus is being spoken of in the book...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in those days when King Ahasuerus sat on the t...</td>\n",
       "      <td>Where was King Ahasuerus' throne? (be specific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in those days when King Ahasuerus sat on the t...</td>\n",
       "      <td>What was in Shushan the citadel? &lt;sep&gt; the thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in those days when King Ahasuerus sat on the t...</td>\n",
       "      <td>In what year of King Ahasuerus' reign did he m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in those days when King Ahasuerus sat on the t...</td>\n",
       "      <td>Where was the throne of his kingdom? &lt;sep&gt; in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context   \n",
       "0  Now it came to pass in the days of Ahasuerus (...  \\\n",
       "1  in those days when King Ahasuerus sat on the t...   \n",
       "2  in those days when King Ahasuerus sat on the t...   \n",
       "3  in those days when King Ahasuerus sat on the t...   \n",
       "4  in those days when King Ahasuerus sat on the t...   \n",
       "\n",
       "                                                  qa  \n",
       "0  Which Ahasuerus is being spoken of in the book...  \n",
       "1  Where was King Ahasuerus' throne? (be specific...  \n",
       "2  What was in Shushan the citadel? <sep> the thr...  \n",
       "3  In what year of King Ahasuerus' reign did he m...  \n",
       "4  Where was the throne of his kingdom? <sep> in ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbe_data = getRawPBEData()\n",
    "pbe_data = rawPBEToTrainingData(pbe_data)\n",
    "pbe_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like the format we want. Thus, we can finally push our data through all the preparation steps. Let's wrap this up into one, succinct function.\n",
    "##### Process Data Function\n",
    "1. Get data in PBE db format\n",
    "2. Make into the input and output format that we need.\n",
    "3. Split dataset\n",
    "4. Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData():\n",
    "  raw = getRawPBEData()\n",
    "  pbe_df = rawPBEToTrainingData(raw)\n",
    "  pbe_dict = dfToSplitDataDict(pbe_df)\n",
    "  pbe_tokens = pbe_dict.map(tokenize, batched=True)\n",
    "  return pbe_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Training w/ Hugging Face"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]c:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['context', 'qa', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['context', 'qa', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['context', 'qa', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "pbe_tokens = processData()\n",
    "print(pbe_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are ready to fine-tune the model. First we specify some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "model_location = './pbe_aqg_potsawee_t5'\n",
    "\n",
    "hyperparameters = Seq2SeqTrainingArguments(\n",
    "  model_location,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  eval_steps=100,\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=100,\n",
    "  save_strategy=\"steps\",\n",
    "  save_steps=200,\n",
    "  learning_rate=4e-5,\n",
    "  per_device_train_batch_size=batch_size,\n",
    "  per_device_eval_batch_size=batch_size,\n",
    "  weight_decay=0.01,\n",
    "  save_total_limit=3,\n",
    "  num_train_epochs=1,\n",
    "  predict_with_generate=True,\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"rouge1\",\n",
    "  report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an explanation for a few of the hyperparameters.\n",
    "* `model_location` is where the trained model will be stored\n",
    "* `evaluation_strategy` can be `\"no\"`, `\"steps\"`, or `\"epoch\"`. This determines whether the model is evaluated during training and how often\n",
    "* `eval_steps` tells the trainer to evaluate the model after every `100` steps. It is required when `evaluation_strategy` is`\"steps\"`\n",
    "* `logging_strategy`, `logging_steps`, `save_strategy`, and `save_steps` all follow the same format and function as `evaluation_strategy` and `eval_steps`\n",
    "* `learning_rate` the initial learning rate\n",
    "* `per_device_train_batch_size` and `per_device_eval_batch_size` refer to how large the batches are per each GPU/CPU\n",
    "* `weight_decay` is normal weight decay - see [explanation](https://vitalflux.com/weight-decay-in-machine-learning-concepts/#:~:text=Weight%20decay%20is%20a%20regularization%20technique%20that%20is%20used%20in,models%2C%20including%20deep%20neural%20networks.)\n",
    "* `num_train_epochs` specifies the number of epochs that should be performed by the trainer\n",
    "* `predict_with_generate` tells the trainer to evaluate the model while training\n",
    "* `metric_for_best_model` specifies the metric used to evaluate the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have a make a data collator with our tokenizer which will split up our data into batches and pad it appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to set up our metric using the Hugging Face `evaluate` library.\n",
    "\n",
    "I have created a custom metric computation function. It is based on the [rogue score](https://huggingface.co/spaces/evaluate-metric/rouge). It:\n",
    "1. decodes prediction tokens into words\n",
    "2. decodes labels after padding\n",
    "3. computes ROUGE scores using the decoded predictions and labels\n",
    "\n",
    "This custom metrics function is a good place to tweak what we want the output to look like. Possibly, we will want to incentivize different outputs as we get better models trained.\n",
    "\n",
    "Getting this metric right is crucial to the training process. Unfortunately, using Python really sucks in this context because you don't know if something is wrong with your metric until you run the training. This became evident in this project when 7 rounds of training (5 hours each) failed during one of its last steps because the metric was not being computed correctly.\n",
    "\n",
    "All the issues were issues of object type and improper formatting of the computed metrics. A statically-typed language would be able to find the issues before even running the code. But with Python, it can take days to debug a simple function like this. The print statements below took **all night** to get. At this point, this function doesn't work yet and future work is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# compute takes a tuple of (predictions, labels)\n",
    "def compute_metrics(eval_pred):\n",
    "  print(\"eval_pred: \", eval_pred)\n",
    "  # eval_pred:  <transformers.trainer_utils.EvalPrediction object at 0x0000018ABBC6ED10>\n",
    "  \n",
    "  predictions, labels = eval_pred\n",
    "  print(\"predictions: \", predictions)\n",
    "  # predictions:  [[    0   363   410 ...     0     0     0]\n",
    "  #  [    0   363    19 ...     0     0     0]\n",
    "  #  [    0   363    19 ...     0     0     0]\n",
    "  #  ...\n",
    "  #  [    0  2645   410 ...     0     0     0]\n",
    "  #  [    0   363   410 ...     9    23     9]\n",
    "  #  [    0   363   410 ... 26783     1     0]]\n",
    "  print(\"labels: \", labels)\n",
    "  # labels:  [[ 125   47   16 ... -100 -100 -100]\n",
    "  #  [ 363  405 4173 ... -100 -100 -100]\n",
    "  #  [ 363   19   45 ... -100 -100 -100]\n",
    "  #  ...\n",
    "  #  [2645 1622   12 ... -100 -100 -100]\n",
    "  #  [   3 2544  520 ... -100 -100 -100]\n",
    "  #  [ 363  410  717 ... -100 -100 -100]]\n",
    "  \n",
    "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "  \n",
    "  # replace -100 in the labels as we can't decode them.\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "  \n",
    "  # rouge expects a newline after each sentence\n",
    "  decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "  decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "  \n",
    "  # compute ROUGE scores\n",
    "  result = metric.compute(predictions=decoded_preds, references=decoded_labels, \n",
    "                          use_stemmer=True, use_aggregator=True, tokenizer=tokenizer)\n",
    "  print(\"result: \", result)\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before training, we need to configure the trainer and point it to our data and pretrained model.\n",
    "\n",
    "We pass in a model initialization function `getPotsaweeModel` so that the trainer will always start with our base model rather than anything else whe may have done training on. We also pass in all the `hyperparameters` that we set up earlier.\n",
    "\n",
    "Also notice that we pass in our tokenized PBE data for training and validation.\n",
    "\n",
    "Finally, we give it the potsawee `tokenizer` and our own `compute_metrics` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "  model_init=getPotsaweeModel,\n",
    "  args=hyperparameters,\n",
    "  train_dataset=pbe_tokens[\"train\"],\n",
    "  eval_dataset=pbe_tokens[\"validation\"],\n",
    "  data_collator=data_collator,\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm still in the process of getting a TensorBoard to work. When I do, we will get to see the training in progress. The board should display in the notebook right below its launching code cell. This code is still here however because without it you usually don't get to see a progress bar below the training block. So it's staying for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1256), started 5:25:52 ago. (Use '!kill 1256' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir f'{model_location}/runs'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 80%|████████  | 100/125 [1:52:13<16:20, 39.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3773, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_pred:  <transformers.trainer_utils.EvalPrediction object at 0x0000018ABBC6ED10>\n",
      "predictions:  [[    0   363   410 ...     0     0     0]\n",
      " [    0   363    19 ...     0     0     0]\n",
      " [    0   363    19 ...     0     0     0]\n",
      " ...\n",
      " [    0  2645   410 ...     0     0     0]\n",
      " [    0   363   410 ...     9    23     9]\n",
      " [    0   363   410 ... 26783     1     0]]\n",
      "labels:  [[ 125   47   16 ... -100 -100 -100]\n",
      " [ 363  405 4173 ... -100 -100 -100]\n",
      " [ 363   19   45 ... -100 -100 -100]\n",
      " ...\n",
      " [2645 1622   12 ... -100 -100 -100]\n",
      " [   3 2544  520 ... -100 -100 -100]\n",
      " [ 363  410  717 ... -100 -100 -100]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1667\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2006\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2003\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m steps_skipped) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[0;32m   2004\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m-> 2006\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   2007\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2008\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2287\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2285\u001b[0m             metrics\u001b[39m.\u001b[39mupdate(dataset_metrics)\n\u001b[0;32m   2286\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2287\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[0;32m   2288\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[1;32mc:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer_seq2seq.py:159\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m gen_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[0;32m    155\u001b[0m     gen_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m gen_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgeneration_num_beams\n\u001b[0;32m    156\u001b[0m )\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gen_kwargs \u001b[39m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 159\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mevaluate(eval_dataset, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix)\n",
      "File \u001b[1;32mc:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2993\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2990\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   2992\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2993\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   2994\u001b[0m     eval_dataloader,\n\u001b[0;32m   2995\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2996\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2997\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2998\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2999\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   3000\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   3001\u001b[0m )\n\u001b[0;32m   3003\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   3004\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3281\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3277\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3278\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[0;32m   3279\u001b[0m         )\n\u001b[0;32m   3280\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3281\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[0;32m   3282\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3283\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m     18\u001b[0m decoded_labels \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(nltk\u001b[39m.\u001b[39msent_tokenize(label\u001b[39m.\u001b[39mstrip())) \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m decoded_labels]\n\u001b[0;32m     20\u001b[0m \u001b[39m# compute ROUGE scores\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m result \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49mdecoded_preds, references\u001b[39m=\u001b[39;49mdecoded_labels, \n\u001b[0;32m     22\u001b[0m                         use_stemmer\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, use_aggregator\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, tokenizer\u001b[39m=\u001b[39;49mtokenizer)\n\u001b[0;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mresult: \u001b[39m\u001b[39m\"\u001b[39m, result)\n\u001b[0;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\evaluate\\module.py:444\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m inputs \u001b[39m=\u001b[39m {input_name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[0;32m    443\u001b[0m \u001b[39mwith\u001b[39;00m temp_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed):\n\u001b[1;32m--> 444\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompute_kwargs)\n\u001b[0;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--rouge\\b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886\\rouge.py:142\u001b[0m, in \u001b[0;36mRouge._compute\u001b[1;34m(self, predictions, references, rouge_types, use_aggregator, use_stemmer, tokenizer)\u001b[0m\n\u001b[0;32m    140\u001b[0m     score \u001b[39m=\u001b[39m scorer\u001b[39m.\u001b[39mscore_multi(ref, pred)\n\u001b[0;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     score \u001b[39m=\u001b[39m scorer\u001b[39m.\u001b[39;49mscore(ref, pred)\n\u001b[0;32m    143\u001b[0m \u001b[39mif\u001b[39;00m use_aggregator:\n\u001b[0;32m    144\u001b[0m     aggregator\u001b[39m.\u001b[39madd_scores(score)\n",
      "File \u001b[1;32mc:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rouge_score\\rouge_scorer.py:135\u001b[0m, in \u001b[0;36mRougeScorer.score\u001b[1;34m(self, target, prediction)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mfor\u001b[39;00m rouge_type \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrouge_types:\n\u001b[0;32m    133\u001b[0m   \u001b[39mif\u001b[39;00m rouge_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrougeL\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    134\u001b[0m     \u001b[39m# Rouge from longest common subsequences.\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     scores \u001b[39m=\u001b[39m _score_lcs(target_tokens, prediction_tokens)\n\u001b[0;32m    136\u001b[0m   \u001b[39melif\u001b[39;00m rouge_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrougeLsum\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    137\u001b[0m     \u001b[39m# Note: Does not support multi-line text.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mget_sents\u001b[39m(text):\n",
      "File \u001b[1;32mc:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rouge_score\\rouge_scorer.py:199\u001b[0m, in \u001b[0;36m_score_lcs\u001b[1;34m(target_tokens, prediction_tokens)\u001b[0m\n\u001b[0;32m    196\u001b[0m   \u001b[39mreturn\u001b[39;00m scoring\u001b[39m.\u001b[39mScore(precision\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, recall\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, fmeasure\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    198\u001b[0m \u001b[39m# Compute length of LCS from the bottom up in a table (DP appproach).\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m lcs_table \u001b[39m=\u001b[39m _lcs_table(target_tokens, prediction_tokens)\n\u001b[0;32m    200\u001b[0m lcs_length \u001b[39m=\u001b[39m lcs_table[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    202\u001b[0m precision \u001b[39m=\u001b[39m lcs_length \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(prediction_tokens)\n",
      "File \u001b[1;32mc:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rouge_score\\rouge_scorer.py:216\u001b[0m, in \u001b[0;36m_lcs_table\u001b[1;34m(ref, can)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, rows \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    215\u001b[0m   \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, cols \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m--> 216\u001b[0m     \u001b[39mif\u001b[39;00m ref[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m can[j \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m]:\n\u001b[0;32m    217\u001b[0m       lcs_table[i][j] \u001b[39m=\u001b[39m lcs_table[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m][j \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    218\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mashu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:239\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[item]\n\u001b[0;32m    238\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encodings \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encodings[item]\n\u001b[0;32m    240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIndexing with integers (to access backend Encoding for a given batch index) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis not available when using Python based tokenizers\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m     )\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training takes approximately 5 hours. As mentioned before, this training has failed seven times due to various errors in the evaluation metric. Unfortunately, we can't try out the model if the training does not complete successfully."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the `compute_metric` function still has to be debugged. One way I plan to do this is to dig through all of Hugging Face's documentation on its `evaluate` library. Metric computation methods have changed recently, so no tutorials I have found online have yet been able to resolve my problem.\n",
    "\n",
    "Otherwise, the model hyperparameters should be adjusted and tested to produce a better model. The current setup is only a proof-of-concept for transfer learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
