{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c3ef31",
   "metadata": {},
   "source": [
    "# Fine-tuning Llama 2\n",
    "Below I use HF libraries to finetune Llama 2 on SQuAD. I use a locally saved base model. I pull my data from [lmqg/squad](https://huggingface.co/datasets/lmqg/qg_squad/viewer/qg_squad/train?row=0) on HF.\n",
    "\n",
    "Setup: Connect to SoC GPU servers. Install dependencies into a conda environment from `aqg_hf_cuda.yml`.\n",
    "Thanks to [brev.dev](https://github.com/brevdev/notebooks/tree/main) for some examples on how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceec8b54-af50-4b60-94d2-51c92d9f1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments \n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94f4a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "models_dir = \"/home/ac/code/aqg/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d7643",
   "metadata": {},
   "source": [
    "### Data\n",
    "Pull data with `datasets`. All you need is a `.jsonl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ea88109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['answer', 'paragraph_question', 'question', 'sentence', 'paragraph', 'sentence_answer', 'paragraph_answer', 'paragraph_sentence'],\n",
      "        num_rows: 75722\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['answer', 'paragraph_question', 'question', 'sentence', 'paragraph', 'sentence_answer', 'paragraph_answer', 'paragraph_sentence'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['answer', 'paragraph_question', 'question', 'sentence', 'paragraph', 'sentence_answer', 'paragraph_answer', 'paragraph_sentence'],\n",
      "        num_rows: 11877\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/lmqg/qg_squad\n",
    "SQuAD = load_dataset(\"lmqg/qg_squad\")\n",
    "print(SQuAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af00fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SQuAD['train']\n",
    "eval_dataset = SQuAD['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55043937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'established beliefs or customs', 'paragraph_question': \"question: What is heresy mainly at odds with?, context: Heresy is any provocative belief or theory that is strongly at variance with established beliefs or customs. A heretic is a proponent of such claims or beliefs. Heresy is distinct from both apostasy, which is the explicit renunciation of one's religion, principles or cause, and blasphemy, which is an impious utterance or action concerning God or sacred things.\", 'question': 'What is heresy mainly at odds with?', 'sentence': 'Heresy is any provocative belief or theory that is strongly at variance with established beliefs or customs .', 'paragraph': \"Heresy is any provocative belief or theory that is strongly at variance with established beliefs or customs. A heretic is a proponent of such claims or beliefs. Heresy is distinct from both apostasy, which is the explicit renunciation of one's religion, principles or cause, and blasphemy, which is an impious utterance or action concerning God or sacred things.\", 'sentence_answer': 'Heresy is any provocative belief or theory that is strongly at variance with <hl> established beliefs or customs <hl> .', 'paragraph_answer': \"Heresy is any provocative belief or theory that is strongly at variance with <hl> established beliefs or customs <hl>. A heretic is a proponent of such claims or beliefs. Heresy is distinct from both apostasy, which is the explicit renunciation of one's religion, principles or cause, and blasphemy, which is an impious utterance or action concerning God or sacred things.\", 'paragraph_sentence': \"<hl> Heresy is any provocative belief or theory that is strongly at variance with established beliefs or customs . <hl> A heretic is a proponent of such claims or beliefs. Heresy is distinct from both apostasy, which is the explicit renunciation of one's religion, principles or cause, and blasphemy, which is an impious utterance or action concerning God or sacred things.\"}\n"
     ]
    }
   ],
   "source": [
    "# view our data\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98725f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data processing functions that produce the actual untokenized input for various training phases\n",
    "\n",
    "def contextAnswer(example, i):\n",
    "  return f\"Select answer: {example['paragraph_sentence'][i]}\\n Answer: {example['answer'][i]}\"\n",
    "\n",
    "def answer(example):\n",
    "  return f\"Answer: {example['answer']}\"\n",
    "\n",
    "def processData(examples):\n",
    "  output_texts = []\n",
    "  for i in range(len(examples['answer'])):\n",
    "    text = contextAnswer(examples, i)\n",
    "    output_texts.append(text)\n",
    "  return output_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e38b3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67849376-1ca2-4ceb-9757-74b31e507e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0b442d2d3146f89cd8e13d67de1f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_location = f\"{models_dir}/llama-hf/7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True,\n",
    "  # we leave the model quantized in 4 bits\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "  bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# load our model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "  base_model_location,\n",
    "  quantization_config=bnb_config,\n",
    "  device_map=\"auto\",\n",
    "  # research what this is and why i need/don't need it\n",
    "  # trust_remote_code=True,\n",
    "  # use_auth_token=True\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# more info: https://github.com/huggingface/transformers/pull/24906\n",
    "base_model.config.pretraining_tp = 1 \n",
    "\n",
    "# load our tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_location)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c83dee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add custom tokens here\n",
    "new_tokens = [\"<hl>\"]\n",
    "vocabulary = tokenizer.get_vocab().keys()\n",
    "for token in new_tokens:\n",
    "    # check to see if new token is in the vocabulary or not\n",
    "    if token not in vocabulary:\n",
    "        tokenizer.add_tokens(token)\n",
    "\n",
    "base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c2717",
   "metadata": {},
   "source": [
    "And setup a train so that we log, save and evaluate every 50 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23aa1e98-34ee-44ba-878d-59c35b3a4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{models_dir}/output/7bTrainedWithHF\"\n",
    "\n",
    "# this sets up training\n",
    "# we log info (is this where we ask to connect to Weights & Biases?)\n",
    "# every 50 steps we save and evaluate\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=50,\n",
    "    max_steps=100, # how much training we want to do\n",
    "    logging_dir=f\"{output_dir}/logs\", # directory for storing logs\n",
    "    # save the model checkpoint every logging step\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50, # how often to save checkpoints\n",
    "    # evaluate the model every logging step\n",
    "    evaluation_strategy=\"steps\", # ??\n",
    "    eval_steps=100, # how often to pause for eval\n",
    "    do_eval=True # do eval at end\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f5edc",
   "metadata": {},
   "source": [
    "We set the config for the Lora adapter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e790d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does this do?\n",
    "# we configure the Lora adapter\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d435a1",
   "metadata": {},
   "source": [
    "(I experimented with higher alpha and r - and found poorer results...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d8ab6ee-01ff-4ba6-83b3-56342eb5041a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac/miniconda3/envs/aqg_hf_cuda/lib/python3.11/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f63eaba6609404d9eaf38ea6cfcf3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75722 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137100f841984daeae29768230496cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the SFTTrainer from HuggingFace's trl\n",
    "max_seq_length = 512\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=processData,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eaa0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatousac\u001b[0m (\u001b[33mpbe_qag\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ac/code/aqg/hfTrain/wandb/run-20231006_023859-sr2gz8bz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pbe_qag/huggingface/runs/sr2gz8bz' target=\"_blank\">valiant-butterfly-3</a></strong> to <a href='https://wandb.ai/pbe_qag/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pbe_qag/huggingface' target=\"_blank\">https://wandb.ai/pbe_qag/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pbe_qag/huggingface/runs/sr2gz8bz' target=\"_blank\">https://wandb.ai/pbe_qag/huggingface/runs/sr2gz8bz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/100 00:19 < 31:54, 0.05 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ac/code/aqg/hfTrain/finetuneSQuAD.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpuserver.cs.southern.edu/home/ac/code/aqg/hfTrain/finetuneSQuAD.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# pass in resume_from_checkpoint=True to resume from a checkpoint\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpuserver.cs.southern.edu/home/ac/code/aqg/hfTrain/finetuneSQuAD.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# when we train, we can see our progress and system info on wandb.ai\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpuserver.cs.southern.edu/home/ac/code/aqg/hfTrain/finetuneSQuAD.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/aqg_hf_cuda/lib/python3.11/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/aqg_hf_cuda/lib/python3.11/site-packages/transformers/trainer.py:1897\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m   1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39;49misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pass in resume_from_checkpoint=True to resume from a checkpoint\n",
    "# when we train, we can see our progress and system info on wandb.ai\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2eb12",
   "metadata": {},
   "source": [
    "## Running inference on a trained model\n",
    "By default, the PEFT library will only save the Qlora adapters. So we need to load the base Llama 2 model from the Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name=\"meta-llama/Llama-2-7b-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc2d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b0caf6",
   "metadata": {},
   "source": [
    "and load the qlora adapter from a checkpoint directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93beb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, \"/root/llama2sfft-testing/Llama-2-7b-hf-qlora-full-dataset/checkpoint-900\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e17d1c",
   "metadata": {},
   "source": [
    "then run some inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"A note has the following\\nTitle: \\nLabels: \\nContent: i love\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
