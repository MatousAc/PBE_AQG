[dataProcessor]
quiet: True

[qagTrainer]
runLevel: test
addCustomTokens: False
ignoreWarnings: True

[paths]
models: /home/ac/code/aqg/models
base: %(models)s/llama-hf/7b
output: %(models)s/output/7bTrainedWithHF
# directory for training logs
# we log info (is this where we ask to connect to Weights & Biases?)
log: %(output)s/logs
data: lmqg/qg_squad

[peft]
# what is this?
# uses LoraConfig
loraAlpha: 16
loraDropout: 0.1
r: 64
bias: none
taskType: CAUSAL_LM

# this sets up training
[trainArgs]
perDeviceTrainBatchSize: 4
gradientAccumulationSteps: 4
learningRate: 2e-4
loggingSteps: 50
# how much training we want to do
maxSteps: 10
# save the model checkpoint every logging step
saveStrategy: steps
# how often to save checkpoints
saveSteps: 50
# evaluate the model every logging step
evaluationStrategy: steps
# how often to pause for eval
evalSteps: 100
# do eval at end
doEval: True

maxSeqLength: 512
