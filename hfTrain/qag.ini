[general]
quiet: True
ignoreWarnings: True
# modes: 'test', 'norm', or 'heavy' 
mode: test
addCustomTokens: False
# packing lets us pack several examples into one training input
# SFTTrainer uses an EOS token to separate examples
packing: False
# dataFormat: (par|sen)(Hl|Tok)(In)_(Out)
# e.g.: parHlSen_Ans, parHlAns_Q
dataFormat: parHlSen_Ans

[paths]
models: /home/ac/code/aqg/models
base: %(models)s/llama-hf/7b
output: %(models)s/output/7bTrainedWithHF
log: %(output)s/logs
data: C:\Users\mashu\Desktop\code\db\qg_squad
; lmqg/qg_squad

[peft]
# parameter-efficient fine-tuning (PEFT) w/ low rank adapter (LoRA)
# trains the difference in weights Î”h on the side of feed-forward
# layers (FFW). makes for faster, lighter training. see https://rb.gy/9gor5
# https://huggingface.co/docs/peft/conceptual_guides/lora#common-lora-parameters-in-peft

# matrix rank: size of matrices "on the side" of FFW
# determines how many parameters get fine tuned
r: 64
# scaling factor
loraAlpha: 16
# should biaas be retained? vals: 'none', 'all' or 'lora_only'
bias: none
loraDropout: 0.1
# causal lm means the lm only sees tokens to the left of what it's predicting
taskType: CAUSAL_LM

# this sets up training
[trainArgs]
perDeviceTrainBatchSize: 4
gradientAccumulationSteps: 4
learningRate: 2e-4
# how many steps of training we want to do
maxTestSteps: 10
maxNormSteps: 1000
maxHeavySteps: 3000
# save the model checkpoint every logging step
saveStrategy: steps
# how often to save & log checkpoints
saveSteps: 50
# evaluate the model every logging step
evaluationStrategy: steps
# how often to pause for eval
evalSteps: 100
# do eval at end
doEval: True
# SFTTrainer automatically reports to wandb if
# it is installed. put 'none' below to turn off
reportTo: wandb
# output length
maxSeqLength: 512
