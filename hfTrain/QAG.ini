[modes]
runLevel: test

[paths]
models: /home/ac/code/aqg/models
base: %(models)/llama-hf/7b
output: %(models)/output/7bTrainedWithHF
# directory for training logs
# we log info (is this where we ask to connect to Weights & Biases?)
log: %(output)/logs

[data]


[peft]
# what is this?
# uses LoraConfig
loraAlpha: 16
loraDropout: 0.1
r: 64
bias: none
taskType: CAUSAL_LM

# this sets up training
[train]
perDeviceTrainBatchSize: 4
gradientAccumulationSteps: 4
learningRate: 2e-4
loggingSteps: 50
# how much training we want to do
maxSteps: 100
# save the model checkpoint every logging step
saveStrategy: steps
# how often to save checkpoints
saveSteps: 50
# evaluate the model every logging step
evaluationStrategy: steps
# how often to pause for eval
evalSteps: 100
# do eval at end
doEval: True

maxSeqLength: 512
