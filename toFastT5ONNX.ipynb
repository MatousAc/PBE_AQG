{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting a torch model to ONNX using FastT5\n",
    "This notebook shows how we can transform PyTorch models into the ONNX format so we can use our model in the browser.\n",
    "\n",
    "We will be using the instructions detailed in the [FastT5 git repo](https://github.com/Ki6an/fastT5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashu\\miniconda3\\envs\\PBE_AQG\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\mashu\\miniconda3\\envs\\PBE_AQG\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from fastT5 import export_and_get_onnx_model, get_onnx_model\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting to onnx... |##########                      | 1/3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting to onnx... |#####################           | 2/3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting to onnx... |################################| 3/3\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up onnx model...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "onnxPath = \"FastT5ONNX/\"\n",
    "\n",
    "model_name = 'potsawee/t5-large-generation-squad-QuestionAnswer'\n",
    "model = export_and_get_onnx_model(model_name,  quantized=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customize name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.rename(\"models\", onnxPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'T5Encoder' object has no attribute 'main_input_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m t_input \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mIn the beginning, God created the heavens and the earth.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m token \u001b[39m=\u001b[39m tokenizer(t_input, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49mtoken[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], attention_mask\u001b[39m=\u001b[39;49mtoken[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m], num_beams\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m output \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(tokens\u001b[39m.\u001b[39msqueeze(), skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\mashu\\miniconda3\\envs\\PBE_AQG\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mashu\\miniconda3\\envs\\PBE_AQG\\lib\\site-packages\\transformers\\generation\\utils.py:1254\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     generation_config\u001b[39m.\u001b[39mpad_token_id \u001b[39m=\u001b[39m eos_token_id\n\u001b[0;32m   1249\u001b[0m \u001b[39m# 3. Define model inputs\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[39m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \u001b[39m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[0;32m   1252\u001b[0m \u001b[39m# otherwise model_input_name is None\u001b[39;00m\n\u001b[0;32m   1253\u001b[0m \u001b[39m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[1;32m-> 1254\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_model_inputs(\n\u001b[0;32m   1255\u001b[0m     inputs, generation_config\u001b[39m.\u001b[39;49mbos_token_id, model_kwargs\n\u001b[0;32m   1256\u001b[0m )\n\u001b[0;32m   1257\u001b[0m batch_size \u001b[39m=\u001b[39m inputs_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1259\u001b[0m \u001b[39m# 4. Define other model kwargs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mashu\\miniconda3\\envs\\PBE_AQG\\lib\\site-packages\\transformers\\generation\\utils.py:512\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_model_inputs\u001b[1;34m(self, inputs, bos_token_id, model_kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39mThis function extracts the model-specific `inputs` for generation.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[39m# 1. retrieve all kwargs that are non-None or non-model input related.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[39m# some encoder-decoder models have different names for model and encoder\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    510\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder\n\u001b[0;32m    511\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 512\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mmain_input_name \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmain_input_name\n\u001b[0;32m    513\u001b[0m ):\n\u001b[0;32m    514\u001b[0m     input_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mmain_input_name\n\u001b[0;32m    515\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mashu\\miniconda3\\envs\\PBE_AQG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'T5Encoder' object has no attribute 'main_input_name'"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "t_input = \"In the beginning, God created the heavens and the earth.\"\n",
    "token = tokenizer(t_input, return_tensors='pt')\n",
    "\n",
    "tokens = model.generate(input_ids=token['input_ids'], attention_mask=token['attention_mask'], num_beams=2)\n",
    "\n",
    "output = tokenizer.decode(tokens.squeeze(), skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use an already exported model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "all or some models don't exists in the model folder, first convert the model! ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m get_onnx_model(onnxPath, quantized\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      4\u001b[0m t_input \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mIn the beginning, God created the heavens and the earth.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mashu\\miniconda3\\envs\\PBE_AQG\\lib\\site-packages\\fastT5\\onnx_models.py:229\u001b[0m, in \u001b[0;36mget_onnx_model\u001b[1;34m(model_name_or_path, quantized)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m    224\u001b[0m         encoder_path\u001b[39m.\u001b[39mexists()\n\u001b[0;32m    225\u001b[0m         \u001b[39mand\u001b[39;00m decoder_path\u001b[39m.\u001b[39mexists()\n\u001b[0;32m    226\u001b[0m         \u001b[39mand\u001b[39;00m init_decoder_path\u001b[39m.\u001b[39mexists()\n\u001b[0;32m    227\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mquantized model don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt exist in the model folder, first quantize the model!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 229\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m    230\u001b[0m         encoder_path\u001b[39m.\u001b[39mexists()\n\u001b[0;32m    231\u001b[0m         \u001b[39mand\u001b[39;00m decoder_path\u001b[39m.\u001b[39mexists()\n\u001b[0;32m    232\u001b[0m         \u001b[39mand\u001b[39;00m init_decoder_path\u001b[39m.\u001b[39mexists()\n\u001b[0;32m    233\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mall or some models don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt exists in the model folder, first convert the model! \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m model_paths \u001b[39m=\u001b[39m encoder_path, decoder_path, init_decoder_path\n\u001b[0;32m    237\u001b[0m model_sessions \u001b[39m=\u001b[39m get_onnx_runtime_sessions(model_paths)\n",
      "\u001b[1;31mAssertionError\u001b[0m: all or some models don't exists in the model folder, first convert the model! "
     ]
    }
   ],
   "source": [
    "model = get_onnx_model(onnxPath, quantized=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "t_input = \"In the beginning, God created the heavens and the earth.\"\n",
    "\n",
    "token = tokenizer(t_input, return_tensors='pt')\n",
    "tokens = model.generate(input_ids=token['input_ids'], attention_mask=token['attention_mask'], num_beams=2)\n",
    "\n",
    "output = tokenizer.decode(tokens.squeeze(), skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PBE_AQG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
