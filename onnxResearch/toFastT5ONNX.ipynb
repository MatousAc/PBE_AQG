{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting a torch model to ONNX using FastT5\n",
    "This notebook shows how we can transform PyTorch models into the ONNX format so we can use our model in the browser.\n",
    "\n",
    "We will be using the instructions detailed in the [FastT5 git repo](https://github.com/Ki6an/fastT5)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to use the [fastT5 environment](https://github.com/huggingface/transformers/issues/15610#issuecomment-1533933968) so that this process works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastT5 import export_and_get_onnx_model, get_onnx_model\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting to onnx... |#####################           | 2/3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting to onnx... |################################| 3/3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[?25h\u001b[?25l\r"
     ]
    }
   ],
   "source": [
    "onnxPath = \"../onnx/fastT5/\"\n",
    "\n",
    "model_name = 'potsawee/t5-large-generation-squad-QuestionAnswer'\n",
    "model = export_and_get_onnx_model(model_name, custom_output_path=onnxPath, quantized=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customize name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.35k/2.35k [00:00<00:00, 1.17MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792k/792k [00:00<00:00, 12.3MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.42M/2.42M [00:00<00:00, 14.3MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.0/21.0 [00:00<00:00, 7.00kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.23k/2.23k [00:00<00:00, 741kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who created the heavens and the earth? God\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "t_input = \"In the beginning, God created the heavens and the earth.\"\n",
    "token = tokenizer(t_input, return_tensors='pt')\n",
    "\n",
    "tokens = model.generate(input_ids=token['input_ids'], attention_mask=token['attention_mask'], num_beams=2)\n",
    "\n",
    "output = tokenizer.decode(tokens.squeeze(), skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, ladies and gentlemen, there you have it! A T5 model that is exported and executed too.\n",
    "\n",
    "In practice, simply execute the python script from `transformers-web` to generate the T5 model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python3 tools/convert_model.py <modelid> <outputdir> <quantize> <testinput>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python scripts/convertModel.py potsawee/t5-large-generation-squad-QuestionAnswer ./pbe/src/assets/fastT5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: [fromFastT5ONNX.ipynb](./fromFastT5ONNX.ipynb) is not currently working properly.\n",
    "\n",
    "Maybe quantizing the model does something? Though it doesn't work even if you don't quantize ðŸ¤·"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how to load a model from a file see [fromFastT5ONNX.ipynb](./fromFastT5ONNX.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PBE_AQG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
