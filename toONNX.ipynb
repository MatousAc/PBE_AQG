{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting a torch model to ONNX\n",
    "This notebook shows how we can transform PyTorch models into the ONNX format so we can use our model in the browser."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, imagine we load the model from somewhere. Below, we use the potsawee model, but in the future we will use our own trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashu\\miniconda3\\envs\\PBE_AQG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we export the model to onnx. There are several things we have to do to make this work.\n",
    "1. We must generate sample inputs for the model using the Tokenizer. According to [the documentation](https://pytorch.org/docs/stable/onnx.html), this is done because to convert the model to the ONNX format, the model needs to be run once with its proper input.\n",
    "2. We should also provide the input and output names for columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashu\\miniconda3\\envs\\PBE_AQG\\Lib\\site-packages\\transformers\\generation\\utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "sample_text = \"In the beginning, God created the heavens and the earth.\"\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "decoder_input_ids = torch.ones_like(inputs[\"input_ids\"])\n",
    "outputs = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], decoder_input_ids=decoder_input_ids)\n",
    "question_answer = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "onnx_path = \"onnxModel/potsawee.onnx\"\n",
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "output_names = [\"output\"]\n",
    "dynamic_axes = {\"input_ids\": {0: \"batch_size\", 1: \"seq_len\"}, \"attention_mask\": {0: \"batch_size\", 1: \"seq_len\"}, \"output\": {0: \"batch_size\", 1: \"seq_len\"}}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the `torch.onnx.export` function to export the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: {'input_ids': tensor([[  86,    8, 1849,    6,  601,  990,    8, 9922,    7,   11,    8, 3596,\n",
      "            5,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"inputs:\", inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mashu\\miniconda3\\envs\\PBE_AQG\\Lib\\site-packages\\transformers\\modeling_utils.py:828: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_mask.shape[1] < attention_mask.shape[1]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, args=tuple(inputs.values()) + (decoder_input_ids,), f=onnx_path, input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that should be it! There should be a `potsawee.onnx` file in the `onnxModel` directory now. You can also see hundreds of `onnx_*` files that are generated. These are part of the model. Don't erase them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PBE_AQG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
